{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.integrate import solve_ivp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = lambda x: np.heaviside(x, 1)\n",
    "sigmoid = lambda x: 1/(1 + np.exp(-x))\n",
    "\n",
    "t0=0\n",
    "tf=10\n",
    "n=100\n",
    "\n",
    "t = np.linspace(t0, tf, n)\n",
    "\n",
    "X = (H(t) - H(t-2))*t - (H(t-2) - H(t-6))*(t-6)*0.5\n",
    "x = interp1d(t, X)\n",
    "\n",
    "\n",
    "# Ideal integrator: y' = u\n",
    "f = lambda t, y: x(t)\n",
    "y0 = np.array([0])\n",
    "\n",
    "# Forward neuron: y -> u\n",
    "K = 10\n",
    "f = lambda t, y: (x(t) - y) * K\n",
    "y0 = np.array([0])\n",
    "\n",
    "# Forward neuron with activation point:\n",
    "K = 100\n",
    "slope = 100\n",
    "Iup = 1.5\n",
    "a = lambda u: sigmoid(slope*(x - Iup)) * u\n",
    "f = lambda t, y: (a(x(t)) - y) * K\n",
    "y0 = np.array([0])\n",
    "\n",
    "# Bistable neuron:\n",
    "K = 10\n",
    "slope = 20\n",
    "Iup = 1.5\n",
    "Idown = 0.5\n",
    "above = lambda x: sigmoid(slope*(x - Iup))\n",
    "below = lambda x: 1-sigmoid(slope*(x - Idown))\n",
    "# Two terms in the derivative where the first one pulls the output up when above the activation threshold,\n",
    "# and the other pulls it down when below. When between the thresholds, the output more or less remains unchanged.\n",
    "f = lambda t, y: (above(x(t))*(1 - y) - below(x(t))*y) * K\n",
    "y0 = np.array([0])\n",
    "\n",
    "solution = solve_ivp(f, (t0, tf), y0, method = \"Radau\") # Radau because equation is stiff\n",
    "y = interp1d(solution.t, solution.y[0])\n",
    "print(solution.t.size)\n",
    "\n",
    "fig = px.line(x=t, y=[x(t), y(t), above(x(t)), below(x(t))], labels={\"x\": \"t\", \"value\": \"activity\"})\n",
    "fig.data[0].name = \"x(t)\"\n",
    "fig.data[1].name = \"y(t)\"\n",
    "fig.add_trace(go.Scatter(x=solution.t, y=solution.y[0], mode=\"markers\", name=\"solution points\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0=0\n",
    "tf=3\n",
    "n=100\n",
    "\n",
    "t = np.linspace(t0, tf, n)\n",
    "\n",
    "X = H(t-1) - H(t-1.5) #0.5*((H(t) - H(t-2))*t - (H(t-2) - H(t-6))*(t-6)*0.5)\n",
    "x = interp1d(t, X)\n",
    "\n",
    "N = 1\n",
    "K = 30\n",
    "tau = K #10\n",
    "slope = 20\n",
    "gain = 1.1\n",
    "above = lambda x, Iup: sigmoid(slope*(x - Iup))\n",
    "below = lambda x, Idown: 1-sigmoid(slope*(x - Idown))\n",
    "# Two terms in the derivative where the first one pulls the output up when above the activation threshold,\n",
    "# and the other pulls it down when below. When between the thresholds, the output more or less remains unchanged.\n",
    "f = lambda t, y: np.array([\n",
    "    (above(x(t) + y[0]/N*gain, 0.6)*(1 - y[0]) - below(x(t) + y[0]/N*gain, 0.05)*y[0]) * K, # + tau*(y[0]/N*gain - y[0]),\n",
    "    #tau*(y[0]/N*gain - y[1]),\n",
    "])\n",
    "\n",
    "initial = np.array([0, 0])\n",
    "solution = solve_ivp(f, (t0, tf), initial, method = \"Radau\") # Radau because equation is stiff\n",
    "\n",
    "n = interp1d(solution.t, solution.y[1]/N)\n",
    "y0 = interp1d(solution.t, solution.y[0])\n",
    "\n",
    "fig = px.line(x=t, y=[x(t), n(t), y0(t)], labels={\"x\": \"t\", \"value\": \"activity\"})\n",
    "fig.data[0].name = \"x(t)\"\n",
    "fig.data[1].name = \"n(t)\"\n",
    "fig.data[2].name = \"y_0(t)\"\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t0=0\n",
    "tf=10\n",
    "n=100\n",
    "\n",
    "t = np.linspace(t0, tf, n)\n",
    "\n",
    "X = 0.1*((H(t) - H(t-2))*t + 1*(H(t-2) - H(t-3)))\n",
    "#X = H(t-1)\n",
    "x = interp1d(t, X)\n",
    "\n",
    "N = 200\n",
    "K = 20\n",
    "slope = 500\n",
    "gain = 0.98\n",
    "\n",
    "above = lambda x, Iup: sigmoid(slope*(x - Iup))\n",
    "below = lambda x, Idown: 1-sigmoid(slope*(x - Idown))\n",
    "\n",
    "Iup = np.linspace(1/(N), 1.0, N)\n",
    "#print(Iup)\n",
    "#print(Iup[0])\n",
    "#print(Iup[1] - Iup[0])\n",
    "#print(Iup[2] - Iup[1])\n",
    "deltaI = 10.0 / N # ensure overlap\n",
    "\n",
    "print(deltaI)\n",
    "\n",
    "f = lambda t, z: np.array([x(t)])\n",
    "solution = solve_ivp(f, (t0, tf), np.array([0]), t_eval=t, method=\"Radau\")\n",
    "z = interp1d(solution.t, solution.y[0])\n",
    "\n",
    "def f(t, y):\n",
    "    # Input is recurrent connections plus network input\n",
    "    I = np.sum(y)/N*gain + x(t)\n",
    "    return (above(I, Iup)*(1-y) - below(I, Iup-deltaI)*y) * K\n",
    "    \n",
    "initial = np.zeros(N)\n",
    "solution = solve_ivp(f, (t0, tf), initial) # Radau because equation is stiff\n",
    "\n",
    "n = interp1d(solution.t, np.sum(solution.y, 0)/N)\n",
    "\n",
    "fig = px.line(x=t, y=[x(t), z(t), n(t)], labels={\"x\": \"t\", \"value\": \"activity\"})\n",
    "fig.data[0].name = \"x(t)\"\n",
    "fig.data[1].name = \"true integral\"\n",
    "fig.data[2].name = \"n(t)\"\n",
    "\n",
    "#for i in range(0, N):\n",
    "#    fig.add_trace(go.Scatter(x=solution.t, y=solution.y[i], name=f\"y_{i}(t)\"))\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
